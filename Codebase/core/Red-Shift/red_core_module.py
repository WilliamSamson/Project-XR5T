import torch
import torch.nn as nn
import torch.optim as optim
import time
from torch.utils.data import Dataset, DataLoader, random_split
from collections import deque
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../Aether'))  # Add the Aether directory to the path
from Aether_v1 import Aether  # Import directly from Aether_v1.py


class MetaLearner(nn.Module):
    def __init__(self, input_dim=4, hidden_dim=32):
        super(MetaLearner, self).__init__()
        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 4)  # Predict: [No Change, Expand, Compress, Prune]

    def forward(self, sequence):
        _, (h_n, _) = self.lstm(sequence)
        out = self.fc(h_n.squeeze(0))
        return torch.softmax(out, dim=1)


class CognitiveCore(nn.Module):
    def __init__(self, input_size, base_hidden_size, output_size, dropout_p=0.5):
        super(CognitiveCore, self).__init__()
        self.fc1 = nn.Linear(input_size, base_hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=dropout_p)
        self.fc2 = nn.Linear(base_hidden_size, output_size)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


class DynamicCognitiveCore(nn.Module):
    def __init__(self, input_size, base_hidden_size, total_hidden_size, output_size, dropout_p=0.5):
        super(DynamicCognitiveCore, self).__init__()
        self.fc1 = nn.Linear(input_size, base_hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=dropout_p)
        self.fc_extra = nn.Linear(base_hidden_size, total_hidden_size)
        self.fc2 = nn.Linear(total_hidden_size, output_size)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc_extra(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

    def prune(self, prune_percentage=0.2):
        with torch.no_grad():
            for param in self.parameters():
                if len(param.shape) == 2:  # Only consider weight parameters
                    grad_norm = param.grad.abs().sum(dim=0)  # Calculate gradient sum for each neuron
                    prune_threshold = torch.topk(grad_norm, int(param.size(1) * prune_percentage), largest=False)[0][-1]
                    prune_mask = grad_norm <= prune_threshold
                    param.data[:, prune_mask] = 0.0  # Prune the neurons
                    print(f"Pruned {prune_percentage * 100}% of neurons in layer {param.size()}")


class SelfOptimizationEngineWithAether:
    def __init__(self, model, input_size, base_hidden_size, output_size, learning_rate=0.0005, weight_decay=1e-4, max_extra_capacity=256, aether=None):
        self.model = model
        self.input_size = input_size
        self.base_hidden_size = base_hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.extra_capacity = 0
        self.max_extra_capacity = max_extra_capacity
        self.aether = aether  # Aether integration

        self.optimizer = optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=weight_decay)
        self.loss_fn = nn.MSELoss()

        # Meta-structures
        self.meta_learner = MetaLearner()
        self.meta_optimizer = optim.Adam(self.meta_learner.parameters(), lr=1e-4)

        self.knowledge_bank = deque(maxlen=20)  # Store past (val_loss, grad_norm, capacity, lr)
        self.patience = 3
        self.improvement_thresh = 0.005

    def train_on_batch(self, x, y):
        self.optimizer.zero_grad()
        output = self.model(x)
        loss = self.loss_fn(output, y)
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def validate(self, dataloader):
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for x, y in dataloader:
                output = self.model(x)
                loss = self.loss_fn(output, y)
                total_loss += loss.item()
        self.model.train()
        return total_loss / len(dataloader)

    def monitor_gradient_norm(self):
        total_grad_norm = 0.0
        count = 0
        for param in self.model.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.data.norm(2).item()
                count += 1
        return total_grad_norm / count if count > 0 else 0.0

    def evaluate_meta_action(self):
        if len(self.knowledge_bank) < 5:
            return 0  # Not enough data

        current_state = torch.tensor(list(self.knowledge_bank), dtype=torch.float32)
        sequence = current_state.unsqueeze(0)
        prediction = self.meta_learner(sequence)
        action = torch.argmax(prediction, dim=1).item()
        return action  # 0: No Change, 1: Expand, 2: Compress, 3: Prune

    def dynamic_architecture_adjustment(self):
        if self.extra_capacity >= self.max_extra_capacity:
            print("Maximum extra capacity reached.")
            return

        increment = 32
        new_extra_capacity = self.extra_capacity + increment
        new_total_hidden = self.base_hidden_size + new_extra_capacity
        old_model = self.model

        new_model = DynamicCognitiveCore(self.input_size, self.base_hidden_size, new_total_hidden, self.output_size)

        # Transfer weights from old model to new model
        new_model.fc1.weight.data.copy_(old_model.fc1.weight.data)
        new_model.fc1.bias.data.copy_(old_model.fc1.bias.data)

        identity = torch.eye(self.base_hidden_size)
        new_weight = torch.zeros(new_total_hidden, self.base_hidden_size)
        new_weight[:self.base_hidden_size, :] = identity
        new_model.fc_extra.weight.data.copy_(new_weight)
        new_model.fc_extra.bias.data.zero_()

        old_total_hidden = self.base_hidden_size + self.extra_capacity if hasattr(old_model, 'fc_extra') else self.base_hidden_size
        new_model.fc2.weight.data[:, :old_total_hidden].copy_(old_model.fc2.weight.data)
        if new_total_hidden > old_total_hidden:
            new_model.fc2.weight.data[:, old_total_hidden:].zero_()
        new_model.fc2.bias.data.copy_(old_model.fc2.bias.data)

        self.model = new_model
        self.extra_capacity = new_extra_capacity
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

    def self_refine(self, epochs, train_loader, val_loader):
        best_val_loss = float('inf')
        epochs_since_improvement = 0

        for epoch in range(epochs):
            start = time.time()
            epoch_loss = 0
            for x, y in train_loader:
                loss = self.train_on_batch(x, y)
                epoch_loss += loss
            avg_train_loss = epoch_loss / len(train_loader)
            val_loss = self.validate(val_loader)
            grad_norm = self.monitor_gradient_norm()
            epoch_time = time.time() - start

            # Log training metrics
            print(f"Epoch {epoch + 1}: Train={avg_train_loss:.4f}, Val={val_loss:.4f}, GradNorm={grad_norm:.4f}, Time={epoch_time:.2f}s")

            self.knowledge_bank.append([
                val_loss,
                grad_norm,
                self.base_hidden_size + self.extra_capacity,
                self.learning_rate
            ])

            if val_loss < best_val_loss - self.improvement_thresh:
                best_val_loss = val_loss
                epochs_since_improvement = 0
            else:
                epochs_since_improvement += 1

            if epochs_since_improvement >= self.patience:
                action = self.evaluate_meta_action()
                if action == 1:
                    print("ðŸ”§ Meta-Learner Trigger: EXPANSION")
                    self.dynamic_architecture_adjustment()
                elif action == 3:
                    print("ðŸ§¹ Meta-Learner Trigger: PRUNING")
                    self.model.prune(prune_percentage=0.2)
                else:
                    print("ðŸ“‰ Meta-Learner Trigger: NO CHANGE")
                epochs_since_improvement = 0


class AdvancedSyntheticDataset(Dataset):
    def __init__(self, num_samples=10000, input_size=5000, output_size=2, noise_std=0.2, device="cpu"):
        self.num_samples = num_samples
        self.input_size = input_size
        self.output_size = output_size
        self.device = device
        self.noise_std = noise_std

        assert input_size >= 4, "Input size must be >= 4"

        self.inputs = torch.randn(num_samples, input_size)
        half = input_size // 2
        first_half, second_half = self.inputs[:, :half], self.inputs[:, half:]

        sine = torch.sin(first_half) + torch.exp(-first_half)
        quad = second_half ** 2 + torch.log1p(torch.abs(second_half))
        s_mean = sine.mean(dim=1, keepdim=True)
        q_mean = quad.mean(dim=1, keepdim=True)
        i1 = s_mean * q_mean
        i2 = s_mean ** 2 + q_mean ** 2

        noise = torch.randn(num_samples, output_size) * noise_std
        self.outputs = torch.cat([s_mean, q_mean], dim=1) + 0.5 * i1 + 0.3 * i2 + noise

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return self.inputs[idx], self.outputs[idx]


def main(input_size=10000, base_hidden_size=64, output_size=2, batch_size=64, num_epochs=100, learning_rate=0.0005):
    torch.manual_seed(42)

    initial_model = CognitiveCore(input_size, base_hidden_size, output_size)

    # Initialize Aether instance with required parameters (from Aether/Aether_v1.py)
    aether_instance = Aether(initial_data={"x": 0.5, "y": 0.3}, threshold=0.15)

    engine_with_aether = SelfOptimizationEngineWithAether(
        model=initial_model,
        input_size=input_size,
        base_hidden_size=base_hidden_size,
        output_size=output_size,
        learning_rate=learning_rate,
        aether=aether_instance
    )

    dataset = AdvancedSyntheticDataset(num_samples=1000, input_size=input_size, output_size=output_size)
    train_size = int(0.8 * len(dataset))
    train_set, val_set = random_split(dataset, [train_size, len(dataset) - train_size])
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=batch_size)

    print("ðŸš€ Initiating Meta-Learning Self Optimization with Aether feedback")
    engine_with_aether.self_refine(num_epochs, train_loader, val_loader)
    print("âœ… Completed Training")


if __name__ == "__main__":
    main(input_size=10000, base_hidden_size=64, output_size=2, batch_size=64, num_epochs=100, learning_rate=0.0005)